version: '1.0'
metadata:
  name: NireonV4 Standard Setup for Explorer Test
  description: Manifest to bootstrap core services and Explorer for testing Gateway magic.
  author: NIREON Team
  created_at: '2025-06-05T10:00:00Z'
  updated_at: '2025-06-07T12:00:00Z' # Note: You may want to update this timestamp

shared_services:
  # The EventBus is a core system dependency, correctly defined.
  event_bus_memory:
    enabled: true
    class: infrastructure.event_bus.memory_event_bus:MemoryEventBus
    port_type: domain.ports.event_bus_port:EventBusPort
    metadata_definition: infrastructure.event_bus.memory_event_bus:MEMORY_EVENT_BUS_METADATA
    config:
      max_history: 1000
      enable_persistence: false
    # config_override was redundant and has been removed for clarity.

  # ParameterService provides LLM settings; it doesn't implement LLMPort itself.
  # The port_type has been removed for clarity as it was potentially confusing.
  parameter_service_global:
    enabled: true
    class: infrastructure.llm.parameter_service:ParameterService
    metadata_definition: infrastructure.llm.parameter_service:PARAMETER_SERVICE_METADATA
    config:
      storage_backend: memory # This seems like a placeholder key, ensure it's used by the class.

  # FrameFactoryService is a core service for the A->F->CE model.
  # Removed confusing port_type as it doesn't implement EventBusPort.
  frame_factory_service:
    enabled: true
    class: application.services.frame_factory_service:FrameFactoryService
    metadata_definition: application.services.frame_factory_service:FRAME_FACTORY_SERVICE_METADATA
    config:
      default_frame_type: epistemic

  # The BudgetManager is crucial for resource governance within Frames.
  budget_manager_inmemory:
    enabled: true
    class: application.services.budget_manager:InMemoryBudgetManager
    port_type: domain.ports.budget_manager_port:BudgetManagerPort
    metadata_definition: application.services.budget_manager:BUDGET_MANAGER_METADATA
    config:
      initial_budgets:
        llm_calls: 10.0
        event_publishes: 100.0
        embedding_calls: 50.0

  # This LLM Router definition is sophisticated and well-defined. No changes needed.
  llm_router_main:
    enabled: true
    class: infrastructure.llm.router:LLMRouter
    metadata_definition: infrastructure.llm.router:LLM_ROUTER_METADATA
    port_type: domain.ports.llm_port:LLMPort
    config:
      id: llm_router_main
      default: nano_default
      routes:
        default: nano_default
        chat_fast: nano_default
        research: nano_default
        analysis: nano_default
        sentinel_axis_scorer: nano_default
      enable_circuit_breakers: true
      enable_health_checks: true
      enable_metrics: true
      health_check_interval: 300
      circuit_breaker_defaults:
        failure_threshold: 5
        recovery_timeout: 60
        success_threshold: 3
        timeout: 30
      models:
        nano_default:
          provider: openai
          model_name_for_api: gpt-4o-mini
          backend: infrastructure.llm.generic_http:GenericHttpLLM
          method: POST
          base_url: https://api.openai.com/v1
          endpoint: /chat/completions
          auth_style: bearer
          auth_token_env: OPENAI_API_KEY
          timeout: 60
          payload_template: |
            {
              "model": "{{ model_name_for_api }}",
              "messages": [
                {"role": "system", "content": "{{ system_prompt }}"},
                {"role": "user",   "content": "{{ prompt }}"}
              ],
              "temperature": {{ temperature }},
              "top_p": {{ top_p }},
              "max_tokens": {{ max_tokens }}
            }
          response_text_path: $.choices[0].message.content
        mock_testing_model:
          provider: mock
          # <<< NOTE: The file list doesn't show a `mock_llm.py` file.
          # This path might need to be corrected to a valid mock implementation.
          backend: infrastructure.llm.mock_llm:MockLLM
          timeout: 5
        openai_gpt4_specific:
          provider: openai
          backend: infrastructure.llm.generic_http:GenericHttpLLM
          method: POST
          base_url: https://api.openai.com/v1
          endpoint: /chat/completions
          auth_style: bearer
          auth_token_env: OPENAI_API_KEY
          timeout: 120
          model_name_for_api: gpt-4o-mini
          payload_template: |
            {
              "model": "{{ model_name_for_api }}",
              "messages": [
                {"role": "system", "content": "{{ system_prompt }}"},
                {"role": "user",   "content": "{{ prompt }}"}
              ],
              "temperature": {{ temperature }},
              "top_p": {{ top_p }},
              "max_tokens": {{ max_tokens }}
            }
          response_text_path: $.choices[0].message.content

  # <<< CHANGE: Enabled EmbeddingPort with a default local implementation.
  # This is critical for Sentinel and other mechanisms that require semantic analysis.
  EmbeddingPort:
    enabled: true
    class: "infrastructure.embeddings.embeddings.EmbeddingAdapter"
    port_type: domain.ports.embedding_port:EmbeddingPort
    config:
      provider: "sentence_transformers"
      model: "all-MiniLM-L6-v2"
      dimensions: 384
      cache_size: 1000

  # <<< CHANGE: Enabled IdeaRepositoryPort with a default local SQLite implementation.
  # This is required for the IdeaService, which is a dependency for Sentinel and others.
  IdeaRepositoryPort:
    enabled: true
    class: "infrastructure.persistence.idea_repository.IdeaRepository"
    port_type: domain.ports.idea_repository_port:IdeaRepositoryPort
    config:
      provider: "sqlite"
      db_path: "runtime/nireon_ideas.db"
      enable_wal_mode: true
      timeout: 30.0

composites:
  mechanism_gateway_main:
    class: application.gateway.mechanism_gateway:MechanismGateway
    metadata_definition: application.gateway.mechanism_gateway:MECHANISM_GATEWAY_METADATA
    enabled: true
    dependencies_override:
      LLMPort: llm_router_main
      ParameterService: parameter_service_global
      FrameFactoryService: frame_factory_service
      EventBusPort: event_bus_memory
      BudgetManagerPort: budget_manager_inmemory

mechanisms:
  explorer_instance_01:
    class: "components.mechanisms.explorer.service.ExplorerMechanism"
    metadata_definition: "components.mechanisms.explorer.service.EXPLORER_METADATA"
    config: "configs/default/mechanisms/{id}.yaml"
    config_override:
      application_rate: 1.0 # Let's make explorer always run for the test too
      max_depth: 3
      exploration_strategy: "depth_first"

  sentinel_instance_01:
    class: "components.mechanisms.sentinel.service.SentinelMechanism"
    metadata_definition: "components.mechanisms.sentinel.metadata.SENTINEL_METADATA"
    config: "configs/default/mechanisms/{id}.yaml"
    config_override:
      trust_threshold: 5.5

  catalyst_instance_01:
    class: "components.mechanisms.catalyst.service:CatalystMechanism"
    metadata_definition: "components.mechanisms.catalyst.metadata:CATALYST_METADATA"
    config: "configs/default/mechanisms/{id}.yaml"
    config_override:
      application_rate: 1.0
      blend_high: 0.4
      anti_constraints_enabled: true
      duplication_check_enabled: true

# These sections remain empty for this test setup, which is correct.
observers: {}
managers: {}
orchestration_commands: {}

environment_overrides:
  development:
    shared_services: {}
  testing:
    shared_services: {}
  production:
    shared_services: {}