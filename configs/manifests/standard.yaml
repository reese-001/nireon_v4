version: "1.0"

metadata:
  name: "NIREON V4 Generic Configuration"
  description: "Production-ready configuration with generic, provider-agnostic naming"
  author: "NIREON Team"
  created_at: "2025-06-05T10:00:00Z"
  updated_at: "2025-06-07T12:00:00Z"

# ----------------------------------------------------------------------
# SHARED SERVICES – only those that are **not** bootstrap-managed.
# Every entry now links to its canonical metadata constant so the
# validator sees the correct epistemic tags.
# ----------------------------------------------------------------------
shared_services:
  EventBusPort:
    enabled: true
    class: "infrastructure.event_bus.memory_event_bus:MemoryEventBus"
    metadata_definition: "infrastructure.event_bus.memory_event_bus:MEMORY_EVENT_BUS_METADATA"
    config:
      max_history: 1000
      enable_persistence: false

  LLMPort:
    enabled: true
    class: "infrastructure.llm.router:LLMRouter"
    metadata_definition: "infrastructure.llm.router:LLM_ROUTER_METADATA"
    config:
      default: "nano_default"

      enable_circuit_breakers: true
      enable_health_checks: true
      enable_metrics: true
      health_check_interval: 300

      circuit_breaker_defaults:
        failure_threshold: 5
        recovery_timeout: 60
        success_threshold: 3
        timeout: 30.0

      models:
        nano_default:
          provider: openai
          backend: "infrastructure.llm.generic_http:GenericHttpLLM"
          method: POST
          base_url: "https://api.openai.com/v1"
          endpoint: "/chat/completions"
          auth_style: bearer
          auth_token_env: "OPENAI_API_KEY"
          timeout: 60
          model_name_for_api: "gpt-4o-mini"
          payload_template: |
            {
              "model": "{{ model_name_for_api }}",
              "messages": [
                {"role": "system", "content": "{{ system_prompt }}"},
                {"role": "user", "content": "{{ prompt }}"}
              ],
              "temperature": {{ temperature }},
              "top_p": {{ top_p }},
              "max_tokens": {{ max_tokens }}
            }
          response_text_path: "$.choices[0].message.content"
          circuit_breaker:
            failure_threshold: 3
            recovery_timeout: 30
            timeout: 45.0

        gemini_pro:
          provider: gemini
          backend: "infrastructure.llm.generic_http:GenericHttpLLM"
          method: POST
          base_url: "https://generativelanguage.googleapis.com"
          endpoint: "/v1beta/models/gemini-pro:generateContent"
          auth_style: header_key
          auth_token_env: "GEMINI_API_KEY"
          auth_header_name: "x-goog-api-key"
          timeout: 60
          model_name_for_api: "gemini-pro"
          payload_template: |
            {
              "contents": [{
                "role": "user",
                "parts": [{"text": "{{ prompt }}"}]
              }],
              "generationConfig": {
                "temperature": {{ temperature }},
                "topP": {{ top_p }},
                "maxOutputTokens": {{ max_tokens }}
              }
            }
          response_text_path: "$.candidates[0].content.parts[0].text"
          circuit_breaker:
            failure_threshold: 8
            recovery_timeout: 120
            timeout: 60.0

        openai_gpt4_specific:
          provider: openai
          backend: "infrastructure.llm.openai_llm:OpenAILLMAdapter"
          model: "gpt-4"
          api_key_env: "OPENAI_API_KEY"
          timeout: 90
          max_retries: 2
          circuit_breaker:
            failure_threshold: 2
            recovery_timeout: 180
            success_threshold: 2
            timeout: 120.0

        mock_testing_model:
          provider: mock
          backend: "bootstrap.bootstrap_helper.placeholders:PlaceholderLLMPortImpl"
          circuit_breaker:
            failure_threshold: 100
            recovery_timeout: 5
            timeout: 5.0

      routes:
        chat_fast: "nano_default"
        quick: "nano_default"
        research: "gemini_pro"
        analysis: "openai_gpt4_specific"
        test: "mock_testing_model"
        mock: "mock_testing_model"

      parameters:
        defaults:
          temperature: 0.7
          max_tokens: 1536
          top_p: 1.0
          system_prompt: "You are a helpful and concise AI assistant."

        by_stage:
          exploration:
            temperature: 0.85
            max_tokens: 2048
            system_prompt: "You are a highly creative and imaginative AI assistant, skilled in divergent thinking and generating novel ideas. Be bold and explore unconventional paths."
          critique:
            temperature: 0.4
            max_tokens: 1024
            system_prompt: "You are a precise and analytical AI assistant. Your role is to critically evaluate the given text, identify flaws, weaknesses, and potential issues. Be objective and thorough."
          synthesis:
            temperature: 0.6
            max_tokens: 2048
            system_prompt: "You are an insightful AI assistant adept at synthesizing information. Combine the provided ideas or concepts into a coherent, well-structured, and enhanced summary or new concept."
          rapid_response:
            temperature: 0.5
            max_tokens: 512
            system_prompt: "You are an efficient AI assistant. Provide clear, concise, and direct responses."
          deep_analysis:
            temperature: 0.3
            max_tokens: 4096
            system_prompt: "You are a thorough AI analyst. Provide comprehensive, detailed analysis with evidence and reasoning."

        by_role:
          innovator:
            top_p: 0.95
            temperature: 0.8
          evaluator:
            temperature: 0.3
            top_p: 0.9
          researcher:
            temperature: 0.4
            max_tokens: 3072
            system_prompt: "You are a meticulous researcher. Provide well-sourced, accurate, and comprehensive information."
          teacher:
            temperature: 0.6
            max_tokens: 2048
            system_prompt: "You are a patient and knowledgeable teacher. Explain concepts clearly with examples and check for understanding."
          developer:
            temperature: 0.2
            max_tokens: 4096
            system_prompt: "You are an expert software developer. Provide clean, efficient, well-documented code with explanations."

  EmbeddingPort:
    enabled: true
    class: "infrastructure.embeddings.embeddings:EmbeddingAdapter"
    metadata_definition: "infrastructure.embeddings.embeddings:EMBEDDING_ADAPTER_METADATA"
    config:
      provider: "sentence_transformers"
      model: "all-MiniLM-L6-v2"
      dimensions: 384
      cache_size: 1000
      timeout: 30
    config_override:
      api_key: "${EMBEDDING_API_KEY}"
      base_url: "${EMBEDDING_API_URL:-https://api.example.com/v1}"

  IdeaRepositoryPort:
    enabled: true
    class: "infrastructure.persistence.idea_repository:IdeaRepository"
    metadata_definition: "infrastructure.persistence.idea_repository:IDEA_REPOSITORY_METADATA"
    config:
      provider: "sqlite"
      db_path: "runtime/ideas_v4.db"
      enable_wal_mode: true
      timeout: 30.0
    config_override:
      connection_string: "${DATABASE_URL:-postgresql://localhost/nireon_v4}"
      pool_size: 5

# ----------------------------------------------------------------------
# MECHANISMS (unchanged, already points to its own metadata constant)
# ----------------------------------------------------------------------
mechanisms:
  explorer_primary:
    enabled: true
    class: "components.mechanisms.explorer.service:ExplorerMechanism"
    metadata_definition: "components.mechanisms.explorer.service:EXPLORER_METADATA"
    config:
      max_depth: 4
      application_rate: 0.6
      exploration_strategy: "depth_first"
      max_variations_per_level: 3
      enable_semantic_exploration: true
      enable_llm_enhancement: true
      creativity_factor: 0.7
      exploration_timeout_seconds: 45.0
    epistemic_tags:
      - "generator"
      - "mutator"
      - "explorer"

observers: {}
managers: {}
composites: {}
orchestration_commands: {}

# ----------------------------------------------------------------------
# ENVIRONMENT OVERRIDES – keep only LLM tweaks; nothing
# bootstrap-managed is declared here.
# ----------------------------------------------------------------------
environment_overrides:
  development:
    shared_services:
      LLMPort:
        enabled: true
        class: "infrastructure.llm.router:LLMRouter"
        config_override:
          default: "mock_testing_model"
          routes:
            default: "mock_testing_model"
            chat_fast: "mock_testing_model"
            quick: "mock_testing_model"
            research: "mock_testing_model"
            analysis: "mock_testing_model"

  testing:
    shared_services:
      IdeaRepositoryPort:
        enabled: true
        class: "bootstrap.bootstrap_helper.placeholders:PlaceholderIdeaRepositoryImpl"
      EventBusPort:
        enabled: true
        class: "bootstrap.bootstrap_helper.placeholders:PlaceholderEventBusImpl"
      LLMPort:
        enabled: true
        class: "infrastructure.llm.router:LLMRouter"
        config_override:
          default: "mock_testing_model"
          routes:
            default: "mock_testing_model"

  production:
    shared_services:
      LLMPort:
        enabled: true
        class: "infrastructure.llm.router:LLMRouter"
        config_override:
          default: "nano_default"
          enable_circuit_breakers: true
          enable_health_checks: true
          enable_metrics: true
          health_check_interval: 120