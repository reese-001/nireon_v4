version: '1.0'
metadata:
  name: NireonV4 Standard Setup for Explorer Test
  description: Manifest to bootstrap core services and Explorer for testing Gateway magic.
  author: NIREON Team
  created_at: '2025-06-05T10:00:00Z'
  updated_at: '2025-06-07T12:00:00Z'

shared_services:
  event_bus_memory:
    enabled: true
    class: infrastructure.event_bus.memory_event_bus:MemoryEventBus
    port_type: domain.ports.event_bus_port:EventBusPort
    metadata_definition: infrastructure.event_bus.memory_event_bus:MEMORY_EVENT_BUS_METADATA
    config:
      max_history: 1000
      enable_persistence: false
    config_override:
      max_history: 1000
  parameter_service_global:
    enabled: true
    class: infrastructure.llm.parameter_service:ParameterService
    port_type: domain.ports.llm_port:LLMPort
    metadata_definition: infrastructure.llm.parameter_service:PARAMETER_SERVICE_METADATA
    config:
      storage_backend: memory
  frame_factory_service:
    enabled: true
    class: application.services.frame_factory_service:FrameFactoryService
    port_type: domain.ports.event_bus_port:EventBusPort
    metadata_definition: application.services.frame_factory_service:FRAME_FACTORY_SERVICE_METADATA
    config:
      default_frame_type: epistemic
  budget_manager_inmemory:
    enabled: true
    class: application.services.budget_manager:InMemoryBudgetManager
    port_type: domain.ports.budget_manager_port:BudgetManagerPort
    metadata_definition: application.services.budget_manager:BUDGET_MANAGER_METADATA
    config:
      initial_budgets:
        llm_calls: 10.0
        event_publishes: 100.0
        embedding_calls: 50.0
  llm_router_main:
    enabled: true
    class: infrastructure.llm.router:LLMRouter
    metadata_definition: infrastructure.llm.router:LLM_ROUTER_METADATA
    port_type: domain.ports.llm_port:LLMPort
    config:
      id: llm_router_main
      default: nano_default
      routes:
        default: nano_default
        chat_fast: nano_default
        research: nano_default
        analysis: nano_default
        sentinel_axis_scorer: nano_default
      enable_circuit_breakers: true
      enable_health_checks: true
      enable_metrics: true
      health_check_interval: 300
      circuit_breaker_defaults:
        failure_threshold: 5
        recovery_timeout: 60
        success_threshold: 3
        timeout: 30
      models:
        nano_default:
          provider: openai
          model_name_for_api: gpt-4o-mini
          backend: infrastructure.llm.generic_http:GenericHttpLLM
          method: POST
          base_url: https://api.openai.com/v1
          endpoint: /chat/completions
          auth_style: bearer
          auth_token_env: OPENAI_API_KEY
          timeout: 60
          payload_template: |
            {
              "model": "{{ model_name_for_api }}",
              "messages": [
                {"role": "system", "content": "{{ system_prompt }}"},
                {"role": "user",   "content": "{{ prompt }}"}
              ],
              "temperature": {{ temperature }},
              "top_p": {{ top_p }},
              "max_tokens": {{ max_tokens }}
            }
          response_text_path: $.choices[0].message.content
        mock_testing_model:
          provider: mock
          backend: infrastructure.llm.mock_llm:MockLLM # This path might be wrong based on your file list
          timeout: 5
        openai_gpt4_specific:
          provider: openai
          backend: infrastructure.llm.generic_http:GenericHttpLLM
          method: POST
          base_url: https://api.openai.com/v1
          endpoint: /chat/completions
          auth_style: bearer
          auth_token_env: OPENAI_API_KEY
          timeout: 120
          model_name_for_api: gpt-4o-mini
          payload_template: |
            {
              "model": "{{ model_name_for_api }}",
              "messages": [
                {"role": "system", "content": "{{ system_prompt }}"},
                {"role": "user",   "content": "{{ prompt }}"}
              ],
              "temperature": {{ temperature }},
              "top_p": {{ top_p }},
              "max_tokens": {{ max_tokens }}
            }
          response_text_path: $.choices[0].message.content
  EmbeddingPort:
    enabled: false
  IdeaRepositoryPort:
    enabled: false

composites:
  mechanism_gateway_main:
    class: application.gateway.mechanism_gateway:MechanismGateway
    metadata_definition: application.gateway.mechanism_gateway:MECHANISM_GATEWAY_METADATA
    enabled: true
    dependencies_override:
      LLMPort: llm_router_main
      ParameterService: parameter_service_global
      FrameFactoryService: frame_factory_service
      EventBusPort: event_bus_memory
      BudgetManagerPort: budget_manager_inmemory

mechanisms:
  explorer_instance_01:
    class: "components.mechanisms.explorer.service.ExplorerMechanism"
    metadata_definition: "components.mechanisms.explorer.service.EXPLORER_METADATA"
    config: "configs/default/mechanisms/{id}.yaml"
    config_override:
      application_rate: 1.0 # Let's make explorer always run for the test too
      max_depth: 3
      exploration_strategy: "depth_first"

  sentinel_instance_01:
    class: "components.mechanisms.sentinel.service.SentinelMechanism"
    metadata_definition: "components.mechanisms.sentinel.metadata.SENTINEL_METADATA"
    config: "configs/default/mechanisms/{id}.yaml"
    config_override:
      trust_threshold: 5.5

  catalyst_instance_01:
    class: "components.mechanisms.catalyst.service:CatalystMechanism"
    metadata_definition: "components.mechanisms.catalyst.metadata:CATALYST_METADATA"
    config: "configs/default/mechanisms/{id}.yaml"
    config_override:
      application_rate: 1.0 # <-- THE FIX: Guarantees the mechanism will run
      blend_high: 0.4
      anti_constraints_enabled: true
      duplication_check_enabled: true

observers: {}
managers: {}
orchestration_commands: {}

environment_overrides:
  development:
    shared_services: {}
  testing:
    shared_services: {}
  production:
    shared_services: {}