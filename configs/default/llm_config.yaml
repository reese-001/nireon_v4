# LLM Configuration
llm:
  # Default model when no specific model is requested
  default: "openai:gpt-4o-mini"
  
  # Default values to use when creating ParameterService
  default_model: "gpt-4o-mini"
  default_temperature: 0.7
  default_max_tokens: 1000
  
  # Model-specific configurations
  models:
    "openai:gpt-4o-mini":
      temperature: 0.7
      max_tokens: 1000
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      
    "openai:gpt-4o":
      temperature: 0.7
      max_tokens: 4000
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      
    "anthropic:claude-3-sonnet":
      temperature: 0.7
      max_tokens: 4000
      top_k: 40
      
  # Providers configuration
  providers:
    openai:
      api_key_env: "OPENAI_API_KEY"
      base_url: "https://api.openai.com/v1"
      default_model: "gpt-4o-mini"
      
    anthropic:
      api_key_env: "ANTHROPIC_API_KEY"
      base_url: "https://api.anthropic.com/v1"
      default_model: "claude-3-sonnet"
      
  # Route definitions - CRITICAL: Add EXPLORATION stage support
  routes:
    # Default catch-all route - MUST BE FIRST or have lowest priority
    - id: "default_route"
      name: "Default Fallback Route"
      stages: ["*"]  # Matches ALL stages including EXPLORATION
      roles: ["*"]   # Matches ALL roles
      model: "gpt-4o-mini"
      temperature: 0.7
      max_tokens: 1000
      priority: 0    # Lowest priority so it's used as fallback
      
    # Explorer-specific route with higher priority
    - id: "exploration_route"
      name: "Exploration Route"
      stages: ["EXPLORATION", "exploration", "Exploration"]  # Handle case variations
      roles: ["idea_generator", "explorer", "*"]
      model: "gpt-4o-mini"
      temperature: 0.8  # Slightly higher for creativity
      max_tokens: 1500
      top_p: 0.95
      frequency_penalty: 0.3
      presence_penalty: 0.3
      priority: 10  # Higher priority than default
      
    # Add the existing stages that your system uses
    - id: "ideation_route"
      name: "Ideation Route"  
      stages: ["IDEATION", "ideation", "Ideation"]
      roles: ["ideator", "generator", "*"]
      model: "gpt-4o-mini"
      temperature: 0.9
      max_tokens: 2000
      priority: 5
      
    - id: "synthesis_route"
      name: "Synthesis Route"
      stages: ["SYNTHESIS", "synthesis", "Synthesis"]  
      roles: ["synthesizer", "*"]
      model: "gpt-4o"  # Use more powerful model for synthesis
      temperature: 0.6
      max_tokens: 3000
      priority: 5
      
    - id: "evaluation_route"
      name: "Evaluation Route"
      stages: ["EVALUATION", "evaluation", "Evaluation"]
      roles: ["evaluator", "critic", "*"]
      model: "gpt-4o-mini"
      temperature: 0.3  # Lower temperature for more consistent evaluation
      max_tokens: 1000
      priority: 5
      
    - id: "refinement_route"
      name: "Refinement Route"
      stages: ["REFINEMENT", "refinement", "Refinement"]
      roles: ["refiner", "*"]
      model: "gpt-4o-mini"
      temperature: 0.5
      max_tokens: 1500
      priority: 5
  
  # Parameter sets that can be referenced by routes
  parameter_sets:
    default:
      temperature: 0.7
      max_tokens: 1000
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      
    exploration:
      temperature: 0.8
      max_tokens: 1500
      top_p: 0.95
      frequency_penalty: 0.3
      presence_penalty: 0.3
      
    creative:
      temperature: 0.9
      max_tokens: 2000
      top_p: 0.95
      frequency_penalty: 0.5
      presence_penalty: 0.5
      
    analytical:
      temperature: 0.3
      max_tokens: 1500
      top_p: 0.9
      frequency_penalty: 0.0
      presence_penalty: 0.0
      
    synthesis:
      temperature: 0.6
      max_tokens: 3000
      top_p: 0.9
      frequency_penalty: 0.2
      presence_penalty: 0.2
  
  # Retry configuration
  retry:
    max_attempts: 3
    initial_delay: 1.0
    backoff_factor: 2.0
    max_delay: 10.0
    
  # Timeout configuration
  timeout:
    default: 30.0
    max: 120.0