# Corrected LLM configuration – *no outer `llm:` wrapper*
# This file is merged into `global_config['llm']` by the ConfigLoader,
# so the keys **default / models / routes** must be at the top level.

default: "nano_default"

models:
  nano_default: &openai_chat
    backend: "infrastructure.llm.generic_http:GenericHttpLLM"
    method: POST
    base_url: "https://api.openai.com/v1"
    endpoint: "/chat/completions"
    auth_style: bearer
    auth_token_env: "OPENAI_API_KEY"
    timeout: 60
    payload_template: |
      {
        "model": "gpt-4o-mini",
        "messages": [
          {"role": "system", "content": "{{ system_prompt }}"},
          {"role": "{{ role }}", "content": "{{ prompt }}"}
        ],
        "temperature": {{ temperature }},
        "top_p": {{ top_p }},
        "max_tokens": {{ max_tokens }}
      }
    response_text_path: "$.choices[0].message.content"

  gemini_pro:
    <<: *openai_chat            # inherit common HTTP structure
    base_url: "https://generativelanguage.googleapis.com"
    endpoint: "/v1beta/models/gemini-pro:generateContent"
    headers:                     # Gemini uses an API‑key header, not bearer
      x-goog-api-key: "${GEMINI_API_KEY}"
    response_text_path: "$.candidates[0].content.parts[0].text"

routes:
  chat_fast:  "nano_default"
  research:   "gemini_pro"
