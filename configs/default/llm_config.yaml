# LLM Configuration
llm:
  # Default model when no specific model is requested
  default: "openai:gpt-4o-mini"
  
  # Default values to use when creating ParameterService
  default_model: "gpt-4o-mini"
  default_temperature: 0.7
  default_max_tokens: 1000
  
  # Model-specific configurations
  models:
    "openai:gpt-4o-mini":
      temperature: 0.7
      max_tokens: 1000
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      
    "openai:gpt-4o":
      temperature: 0.7
      max_tokens: 4000
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
      
    "anthropic:claude-3-sonnet":
      temperature: 0.7
      max_tokens: 4000
      top_k: 40
      
  # Providers configuration
  providers:
    openai:
      api_key_env: "OPENAI_API_KEY"
      base_url: "https://api.openai.com/v1"
      default_model: "gpt-4o-mini"
      
    anthropic:
      api_key_env: "ANTHROPIC_API_KEY"
      base_url: "https://api.anthropic.com/v1"
      default_model: "claude-3-sonnet"
      
  # Route definitions
  routes:
    # Default catch-all route - MUST BE FIRST or have lowest priority
    - id: "default_route"
      name: "Default Fallback Route"
      stages: ["*"]  # Matches ALL stages
      roles: ["*"]   # Matches ALL roles
      model: "gpt-4o-mini"
      temperature: 0.7
      max_tokens: 1000
      priority: 0    # Lowest priority so it's used as fallback
      
    - id: "exploration_route"
      name: "Exploration Route"
      stages: ["EXPLORATION", "exploration", "Exploration"]
      roles: ["idea_generator", "explorer", "*"]
      model: "gpt-4o-mini"
      temperature: 0.8
      max_tokens: 1500
      top_p: 0.95
      frequency_penalty: 0.3
      presence_penalty: 0.3
      priority: 10
      
    - id: "ideation_route"
      name: "Ideation Route"  
      stages: ["IDEATION", "ideation", "Ideation"]
      roles: ["ideator", "generator", "*"]
      model: "gpt-4o-mini"
      temperature: 0.9
      max_tokens: 2000
      priority: 5
      
    - id: "synthesis_route"
      name: "Synthesis Route"
      stages: ["SYNTHESIS", "synthesis", "Synthesis"]  
      roles: ["synthesizer", "*", "math_explainer"] # Added role for PrincipiaAgent
      model: "gpt-4o"
      temperature: 0.6
      max_tokens: 3000
      priority: 5
      
    - id: "evaluation_route"
      name: "Evaluation Route"
      stages: ["EVALUATION", "evaluation", "Evaluation", "CRITIQUE"] # Added CRITIQUE for Sentinel
      roles: ["evaluator", "critic", "*", "sentinel_evaluator"] # Added role for Sentinel
      model: "gpt-4o-mini"
      temperature: 0.3
      max_tokens: 1000
      priority: 5
      
    - id: "refinement_route"
      name: "Refinement Route"
      stages: ["REFINEMENT", "refinement", "Refinement"]
      roles: ["refiner", "*"]
      model: "gpt-4o-mini"
      temperature: 0.5
      max_tokens: 1500
      priority: 5
  
  # Parameter sets for the ParameterService
  parameters:
    defaults:
      temperature: 0.7
      max_tokens: 1000
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
    by_stage:
      exploration:
        temperature: 0.8
        max_tokens: 1500
        top_p: 0.95
        frequency_penalty: 0.3
        presence_penalty: 0.3
      creative:
        temperature: 0.9
        max_tokens: 2000
        top_p: 0.95
        frequency_penalty: 0.5
        presence_penalty: 0.5
      analytical:
        temperature: 0.3
        max_tokens: 1500
        top_p: 0.9
        frequency_penalty: 0.0
        presence_penalty: 0.0
      synthesis:
        temperature: 0.6
        max_tokens: 3000
        top_p: 0.9
        frequency_penalty: 0.2
        presence_penalty: 0.2
      critique:
        temperature: 0.2
        max_tokens: 1000
        top_p: 1.0
        frequency_penalty: 0.0
        presence_penalty: 0.0

  retry:
    max_attempts: 3
    initial_delay: 1.0
    backoff_factor: 2.0
    max_delay: 10.0
    
  timeout:
    default: 60.0
    max: 120.0